{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a845464f-ed5e-44f0-8dba-0a8e10081fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-16 23:49:52.640682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import spacy\n",
    "import json\n",
    "import io,re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2d3794-08f0-4be6-a4d9-fdba26e7adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving the class label to index\n",
    "INDEX_TO_LABEL_FILE_PATH = \"../metadata/index_to_label.json\"\n",
    "\n",
    "##Train data path\n",
    "DATA_PATH = \"../data/reviews_prepared_sample.txt\"\n",
    "\n",
    "##Path to save the model\n",
    "MODEL_PATH = \"../model/classifier.p\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879989e-d15a-4ca8-ab35-96f86a9a0816",
   "metadata": {},
   "source": [
    "<h1> Approach: </h1>\n",
    "<h5>Build an SVM based multi-class classifier<br>The dataset used is a crawl of playstore reviews of an APP named \"CLUE\"</h5>\n",
    "<h5>All the ratings 4,5 are bucketed to POSITIVE</h5>\n",
    "<h5>All the ratings 3 are bucketed to NEUTRAL</h5>\n",
    "<h5>All the ratings 1,2 are bucketed to NEGATIVE</h5>\n",
    "\n",
    "<h5>Apply basic preprocessing like cleaning up special characters, convert every word to lowercase etc</h5>\n",
    "<h5>(The data is build on sample of 2000 reviews)</h5>\n",
    "\n",
    "<h4>Note: Text classification is a sequence classification problem and the best models for this are the model which can make use of the sequential information. So models like RNN, Bidirectional LSTMs are expected to work better than SVM. SVMs are good place to start because its much easier to train and produce fast inferences </h4>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb7068-9e69-4db0-8a97-33c6d01e8d36",
   "metadata": {},
   "source": [
    "<h1> Data distribution:</h1>\n",
    "POSITIVE_SAMPLES:1798 <br>\n",
    "NEGATIVE_SAMPLES:143 <br>\n",
    "NEUTRAL_SAMPLES:58 <br>\n",
    "So its heavily biased, towards Positive, however the the idea is just to show case the model training<br>\n",
    "F1 is used as the evaluation metric <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff773073-bc5d-45e7-bf2b-108dcbad7b10",
   "metadata": {},
   "source": [
    "![title](../artifacts/histogram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "109b42c9-d304-4c87-96b1-64f914c556db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model\n",
      "Done loading spacy model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"Loading spacy model\")\n",
    "spacy_model = spacy.load(\"en_core_web_md\")\n",
    "print (\"Done loading spacy model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58a49cd3-511d-48e4-a638-940912376fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We use Spacy to do the heavy lifting of converting strings to vectors\n",
    "#One exmple could be \n",
    "spacy_model(\"How are you\").vector\n",
    "#spay basically averages out teh vectors of each word to produce a 300 dimension representation of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25cdcf8b-b00e-448d-b6e9-5d6bd5ad0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to save index_to_label mappings to be used for inference\n",
    "def save_index_to_label_mapping(label_indexes,le):\n",
    "    \"\"\"Save index_to_label mappings to be used for inference.\n",
    "            @param label_indexes  indexes of the classes (n labels for n-class classification)\n",
    "            @param le --> is the labelencoder used for preparing data\n",
    "\n",
    "            returns () --> Just saves the mapping\"\"\"\n",
    "    index_to_label_dict = {}\n",
    "    label_indexes_list = list(set(label_indexes))\n",
    "    labels_list = le.inverse_transform(label_indexes_list)\n",
    "    for i in range(len(labels_list)):\n",
    "        index_to_label_dict[str(label_indexes_list[i])] = labels_list[i]\n",
    "\n",
    "    with open(INDEX_TO_LABEL_FILE_PATH, 'w') as outfile:\n",
    "        json.dump(index_to_label_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a61f550-9ad3-4b21-8a3f-5498aa500779",
   "metadata": {},
   "outputs": [],
   "source": [
    "##One of the preprocessing step\n",
    "def lemmatize(text):\n",
    "    \"\"\"Lemmatizes the input sentence\n",
    "            @param text : The input sentence which is to be lemmatized\n",
    "\n",
    "            returns lemmatized sentence\"\"\"\n",
    "    \n",
    "    sent = []\n",
    "    doc = spacy_model(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90f5db-d0db-4f46-bb42-55f9ffeae0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_sentence(sent):\n",
    "     \"\"\"Remove all special characters except space,?\n",
    "            @param sent : The input sentence which is to be cleaned\n",
    "\n",
    "            returns cleaned sentence : lowercase, special symbols removed, adjascent spaces removed \"\"\"\n",
    "    \n",
    "    sent = sent.lower() \n",
    "    sent = sent.replace('\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9 ]+', ' ', sent)\n",
    "    sent = re.sub(' +', ' ', sent)      #Removing adjascent spaces\n",
    "    return sent.strip(\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e57f4399-cc01-45f4-b823-179609901b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Load X, Y from the data file \n",
    "            Applies a basic preprocessing pipeline which cleans, lemmatizes the document  : \n",
    "            returns X,Y : Both are lists \"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    with io.open(DATA_PATH,\"r\",encoding=\"utf-8\") as f:\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for line in f:\n",
    "            if(line.__contains__(\",,,\") and len(line.split(\",,,\")) == 2):\n",
    "                line_splitted = line.split(\",,,\")\n",
    "                x = line_splitted[0].strip()\n",
    "                y = line_splitted[1].strip()\n",
    "\n",
    "                x_cleaned = clean_sentence(x)\n",
    "\n",
    "                x_lemmatized = lemmatize(x_cleaned)\n",
    "                x_data.append(np.array(spacy_model(x_lemmatized).vector))\n",
    "                y_data.append(y)\n",
    "    return x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "058fe454-509f-4319-9fd5-cf6b2b4d82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_config():\n",
    "    \"\"\"Basic config for grid search \n",
    "            returns dict which contains kernel, C which needs to be tried \"\"\"\n",
    "    config = {\n",
    "        \"kernel\": \"linear\",\n",
    "        \"C\": [1,2,5,10,20,100]\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf75377c-a7a8-4d4c-b05f-8af9469ff546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Core training logic - Saves the trained classifier\n",
    "            returns Nothing \"\"\"\n",
    "    X, labels = get_data()\n",
    "    X, labels = shuffle(X,labels)\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "\n",
    "\n",
    "    save_index_to_label_mapping(y,le)\n",
    "\n",
    "    sklearn_config = get_svm_config()\n",
    "    C = sklearn_config.get(\"C\")\n",
    "    kernel = sklearn_config.get(\"kernel\")\n",
    "\n",
    "    params_grid = [{\"C\": C, \"kernel\": [str(kernel)]}]\n",
    "\n",
    "    clf = GridSearchCV(SVC(probability=True, class_weight='balanced',decision_function_shape='ovr'),\n",
    "                            param_grid=params_grid,\n",
    "                            cv=5, scoring='f1_weighted', verbose=1)\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    print(\"best params : {} , best F1: {}\".format(clf.best_params_,clf.best_score_))\n",
    "    f = open(MODEL_PATH, 'wb')\n",
    "    pickle.dump(clf, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10c746c6-0563-436c-9bce-07a7e2845925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "best params : {'C': 2, 'kernel': 'linear'} , best F1: 0.856120724635808\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
